\documentclass[12pt]{article}
\usepackage{fullpage,amssymb,url}
\setlength\parindent{0pt}

% Make each "\section{}" become "Problem N"
\renewcommand{\thesection}{Problem \arabic{section}}

\begin{document}

\begin{center}
{\Large CS 245 Winter 2020 Assignment 2 -- Part I}
\end{center}

By turning in this assignment, I agree to the Stanford honor code and declare
that all of this is my own work.

\section*{Instructions}
You will be writing Relational Algebra for SQL queries before and after
they are optimized by the Catalyst, Spark's SQL optimizer.

\begin{enumerate}
    \item Start a \texttt{spark-shell} session and load the Cities and Countries
        tables, as shown in \texttt{a2\_starter.scala}.
        We suggest you copy-paste the loading code into your spark shell.
        (You can also have the shell run all the commands in the file
        for you with \texttt{spark-shell -i a2\_starter.scala}).
    \begin{itemize}
        \item Run \texttt{SPARK\_233\_HOME/bin/spark-shell}
        from the \texttt{part1/} directory
        (where \\
        \texttt{SPARK\_233\_HOME} is the directory where you
        downloaded and unzipped Spark 2.3.3).

    \end{itemize}

    \item Examine \texttt{Cities.csv} and \texttt{Countries.csv}.
        Observe the output of \texttt{printSchema} on the dataframes
        representing each table (as in the starter code).
        \texttt{temp} indicates average temperature in Celsius and
        \texttt{pop} is the country's population in millions.
    \item For each of the Problem sections below:
    \begin{enumerate}
        \item Think about what the given SQL query does.
        \item Run the query in \texttt{spark-shell} and save the results to a dataframe.
        \item Run \texttt{.show()} on the dataframe to inspect the output.
        \item Run \texttt{.explain(true)} on the dataframe to see Spark's
            query plans.
        \item Write Relational Algebra for the Analyzed Logical Plan
            and for the Optimized Logical Plan, in the space provided
            for each problem.
        \item Write a brief explantation (1-3 sentences) describing why the optimized
            plan differs from the original plan, or, why they are both the same.
    \end{enumerate}
\end{enumerate}

Use the Relational Algebra (RA) notation as introduced in Lecture 6 on Query Execution.
The output of Spark's query plans does not necessarily map perfectly to our RA syntax.
One of the tasks of this assignment is to think
critically about the plans that Spark produces and how they should map
to RA.

Below are a couple examples of simplifying assumptions you can make.
You are welcome to make other reasonable assumptions (if you're not sure,
feel free to ask during OH or post on piazza).
\begin{itemize}
    \item The pound + number suffix of fields
        (e.g. the \verb|#12| in \verb|city#12|)
        in the query plans are
        used by Spark to uniquely determine references to fields.
        This is because a single SQL query can, for instance, have multiple
        fields named \verb|city| (from aliasing or in subqueries).
        You should ignore the field number and just use the name in your
        RA expressions.
        E.g. treat \verb|city#12| as just \verb|city|.
    \item \verb|cast(4 as double)| can be just \verb|4.0|
    \item You can omit \texttt{isnotnull} from your select ($\sigma$) predicates.
\end{itemize}

\textbf{NOTE}:
We have provided two example queries and their valid corresponding solutions below.
Please examine them carefully, as they provide hints and guidance for solving the rest of the problems.

\section*{Example 1}
\begin{verbatim}
SELECT city
FROM Cities
\end{verbatim}

\subsection*{Analyzed Logical Plan}
$\pi_{city}(Cities)$

\subsection*{Optimized Logical Plan}
$\pi_{city}(Cities)$

\subsection*{Explanation}
The analyzed and optimized plans are the exact same because there is no
logical optimization for projecting a single column from a table.

\newpage

\section*{Example 2}
\begin{verbatim}
SELECT *
FROM Cities
WHERE temp < 5 OR true
\end{verbatim}

\subsection*{Analyzed Logical Plan}
$\sigma_{temp < 5 \vee true}(Cities)$

\subsection*{Optimized Logical Plan}
$Cities$

\subsection*{Explanation}
$temp < 5 \vee true = true$, so $\sigma$ selects every row,
which is the same as the relation \verb|Cities| itself.

That is:
$\sigma_{temp < 5 \vee true}(Cities) = \sigma_{true}(Cities) = Cities$

\newpage

\section{}
\begin{verbatim}
SELECT country, EU
FROM Countries
WHERE coastline = "yes"
\end{verbatim}

\subsection*{Analyzed Logical Plan}

$\prod_{country, EU}(\sigma_{coastline = ``yes"}(Countries))$

\subsection*{Optimized Logical Plan}

$\prod_{country, EU}(\sigma_{isnotnull(coastline) \wedge coastline = ``yes"}(Countries))$

\subsection*{Explanation}

The optimizer adds $isnotnull(coastline)$ to the predicate. The $ null$ check filters out rows that have $coastline$ column set to $null$. $null$ checks are faster to test compared to string equality checks i.e. $coastline = ``yes"$. The underlying Data Source also can avoid disk IOs if it uses Run Length Encoding or other compression algorithms that allow it to skip rows with certain values without reading the whole block. We will see this benefit when we run the query over tables that have a lot of rows with $null$ values in the $coastline$ column. This check doesn't help reduce IOs if the column is always populated with a non-$null$ value.
\newpage

\section{}
\begin{verbatim}
SELECT city
FROM (
    SELECT city, temp
    FROM Cities
)
WHERE temp < 4
\end{verbatim}

\subsection*{Analyzed Logical Plan}

$\prod_{city}(\sigma_{(int)temp < 4}(\prod_{city, temp}(Cities)))$

\subsection*{Optimized Logical Plan}

$\prod_{city}(\sigma_{isnotnull(temp) \wedge ((int)temp < 4)}(Cities))$

\subsection*{Explanation}

The query uses a sub-query to project $city$ and $temp$ columns from the $Cities$ table and later filters rows $temp < 4$. Finally the query only projects the $city$ column. The optimizer makes 3 changes to the query:

\begin{enumerate}


\item The optimizer removes the sub-query as it is redundant. The sub-query unnecessarily forces Spark to read \textit{all} blocks from disk and materialize rows in memory only to later find that those rows may not match the filter criteria $temp < 4$. Removing the sub-query does not change the outcome of the result and avoids unnecessary IOs.

\item The optimizer introduces the $isnotnull(temp)$ check \textit{before} casting it to $int$. Performing this condition check filters rows where the $temp$ column is $null$. This can benefit tables that have a large number of $null$s. The underlying DataSource plugin can efficiently skip large number of rows if it uses $Run Length Encoding$ or similar compression algorithm where it doesn't need to read the actual block to filter on the value. This check doesn't help reduce IOs if the column is always populated with a non-$null$ value.

\item The optimizer only projects the $city$ column. Depending on the underlying Data Source, we can entirely avoid the cost of reading any columns except $city$ and $temp$.

\end{enumerate}

\newpage

\section{}
\begin{verbatim}
SELECT *
FROM Cities, Countries
WHERE Cities.country = Countries.country
    AND Cities.temp < 4
    AND Countries.pop > 6
\end{verbatim}

\subsection*{Analyzed Logical Plan}

$\prod(\sigma_{(((Countries.country = Cities.country) \wedge ((int)temp < 4))  \wedge ((int)pop > 6))}(Cities \bowtie Countries))$



\subsection*{Optimized Logical Plan}

$\sigma_{((isnotnull(temp) \wedge ((int)temp < 4)) \wedge isnotnull(country))}(Cities) \bowtie \\
\sigma_{((isnotnull(pop) \wedge ((int)pop > 6)) \wedge isnotnull(country))}(Countries)$

\subsection*{Explanation}

This query is performing a join between $Cities$ and $Countries$ table on the $country$ column. Later it only selects the rows that have $temp < 4 \wedge pop > 6$. In this case the analyzed logical plan first performs a natural join on the $country$ column and then applies the predicate to filter the rows. This can be inefficient so the optimizer performs 3 optimizations:

\begin{enumerate}
\item The optimizer eliminates the explicit $\prod$ operator. We're reading all columns anyway.

\item The optimizer pushes down the predicate that is applicable for each table. This reduces the overall number of IOs and rows that Spark needs to join. For example, $\sigma_{(int)temp < 4)}(Cities)$ may produce a much smaller result set assuming the predicate is highly selective.


\item The optimizer introduces $isnotnull$ check to both $temp$ and $pop$ column allowing the underlying Data Source to skip rows that have null values in those columns. This check doesn't help reduce IOs if the column is always populated with a non-$null$ value.

\item Finally, the optimizer adds $isnotnull(country)$ to the predicates to filter out records that have $null$ values in the $country$ column. Since this column is used for joining the two tables it has to be non $null$. This reduces the overall number of records that Spark needs to join.

\end{enumerate}

By adding $isnotnull$ checks and pushing down predicates the optimizer can greatly reduce the number of records that need to be read and joined. This saves on IO, CPU and Memory cost.
\newpage

\section{}
\begin{verbatim}
SELECT city, pop
FROM Cities, Countries
WHERE Cities.country = Countries.country
    AND Countries.pop > 6
\end{verbatim}

\subsection*{Analyzed Logical Plan}

$\prod_{Cities.city, Countries.pop}(\sigma_{Cities.country = Countries.country \wedge (int) Countries.pop > 6}(Cities \bowtie Countries))$

\subsection*{Optimized Logical Plan}

$\prod_{Cities.city, Countries.pop}((\prod_{city, country}(\sigma_{isnotnull(country)}(Cities))) \bowtie \\
 (\prod_{country, pop}(\sigma_{(isnotnull(pop) \wedge ((int) pop > 6)) \wedge isnotnull(country)}(Country))))$


\subsection*{Explanation}

In this join query, the optimizer chooses to push down the predicates, employs $null$ checks and projects only the $city$, $country$, $pop$ columns from their respective tables.

\begin{enumerate}

\item As noted in previous problems the predicate pushdown and $null$ check can result in big IO savings as the underlying Data Source can skip a lot of blocks if it uses Run Length Encoding or similar compression schemes where it doesn't need to read the block to skip certain values. This check doesn't help reduce IOs if the column is always populated with a non-$null$ value.

\item After the predicate is applied we apply $\prod_{city, country}$ and $\prod_{country, pop}$ to the respective tables. If the underlying Data Source is columnar then it can avoid reading and materializing other columns in memory. Therefore the projections can improve the performance.

\item Finally, the top level projection $\prod_{Cities.city, Countries.pop}$ only projects the columns that are requested.

\end{enumerate}

\newpage

\section{}
\begin{verbatim}
SELECT *
FROM Countries
WHERE country LIKE "%e%d"
\end{verbatim}

\subsection*{Analyzed Logical Plan}
$\prod_{country, pop, EU, coastline}(\sigma_{country LIKE ``\%e\%d"}(Countries))$

\subsection*{Optimized Logical Plan}
$\sigma_{isnotnull(country) \wedge country LIKE ``\%e\%d"}(Countries)$

\subsection*{Explanation}

Here the optimizer eliminates the projection as we're projecting all columns. It also adds $isnotnull(country)$ to the predicate. The $null$ check can significantly reduce the IOs to filter out records especially if the underlying Data Source plugin is using Run Length Encoding or similar compression. This check doesn't help reduce IOs if the column is always populated with a non-$null$ value.

\newpage

\section{}
\begin{verbatim}
SELECT *
FROM Countries
WHERE country LIKE "%ia"
\end{verbatim}

\subsection*{Analyzed Logical Plan}
$\prod_{country, pop, EU, coastline}(\sigma_{country LIKE ``\%i\%a"}(Countries))$


\subsection*{Optimized Logical Plan}
$\sigma_{isnotnull(country) \wedge EndsWith(country, ``ia")}(Countries)$


\subsection*{Explanation}

\begin{enumerate}


\item Here the optimizer eliminates the projection as we're projecting all columns. 

\item It also adds $isnotnull(country)$ to the predicate. The $null$ check can significantly reduce the IOs to filter out records especially if the underlying Data Source plugin is using Run Length Encoding or similar compression. This check doesn't help reduce IOs if the column is always populated with a non-$null$ value.

\item The optimizer also prefers the specialized $EndsWith$ over the LIKE clause. The LIKE clause results into a full regular expression evaluation. Comparatively, prefix and postfix matches can be implemented cheaply.
\end{enumerate}

\newpage

\section{}
\begin{verbatim}
SELECT t1 + 1 as t2
FROM (
    SELECT cast(temp as int) + 1 as t1
    FROM Cities
)
\end{verbatim}

\subsection*{Analyzed Logical Plan}
$\prod_{t1 + 1 \rightarrow t2} ( \prod_{(int)temp + 1 \rightarrow t1} (Cities) )$

\subsection*{Optimized Logical Plan}
$\prod_{(int)temp + 2 \rightarrow t2}(Cities)$

\subsection*{Explanation}

The given query has a sub-query which casts the $temp$ column to an Integer and adds 1 storing the result in a column $t1$. The query subsequently reads this column value and adds 1 to the resulting sum $t2$. In this case the optimizer performs the following optimizations leading to a different Optimized Logical Plan:

\begin{enumerate}
\item Eliminates the sub-query simplifying it to $(temp + 1) + 1$ in the query
\item Eliminating the sub-query leads to an algebraic expression of the form $(temp + 1) + 1$ which can be simplified to $temp + 2$.
\end{enumerate}

This can be accomplished because the original query casts the $temp$ column to an Integer. Integer sum is associative.
\newpage

\section{(Extra Credit -- purely optional)}
\begin{verbatim}
SELECT t1 + 1 as t2
FROM (
    SELECT temp + 1 as t1
    FROM Cities
)
\end{verbatim}

\subsection*{Analyzed Logical Plan}
$\prod_{t1 + (double)1 \rightarrow t2} ( \prod_{(double) temp + (double) 1 \rightarrow t1} (Cities) )$

\subsection*{Optimized Logical Plan}
$\prod_{(((double) temp + 1.0) + 1.0) \rightarrow t2}(Cities)$

\subsection*{Explanation}

This query should be optimized similar to the one in Problem 7 but it isn't. 

\begin{enumerate}
\item The key difference is that Problem 7's query treats the $temp$ column as an Integer while in Problem 8, Spark's Type Inference identifies it as a Double (Floating point value).

\item As we all know floating point arithmetic is not necessarily associative[1]. Therefore the optimizer cannot simplify this expression. This is confirmed by the fact that the optimizer's \texttt{ReorderAssociativeOperator}[2] code only optimizes Integral types[3].
\end{enumerate}

Of note is the following quote from ``What Every Computer Scientist Should Know About Floating-Point Arithmetic (Appendix D)"[1]. 

\begin{quote}
Due to roundoff errors, the associative laws of algebra do not necessarily hold for floating-point numbers. For example, the expression (x+y)+z has a totally different answer than $x+(y+z)$ when $x = 10^{30}$, $y = -10^{30}$ and $z = 1$ (it is 1 in the former case, 0 in the latter). \textit{The importance of preserving parentheses cannot be overemphasized.} [emphasis added]
\end{quote}

As you can see, Spark's optimizer does not even simplify the expression to remove the parentheses from $(((double)temp + 1.0) + 1.0)$.


\begin{tiny}
\begin{verbatim}
[1] https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html
[2] https://github.com/apache/spark/blob/v2.3.3/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala#L156
[3] https://github.com/apache/spark/blob/v2.3.3/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala#L188
\end{verbatim}
\end{tiny}

\end{document}
